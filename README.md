# University of Leeds LecturerÂ AIâ€‘SkillÂ Scraper

---

## ğŸ“– Table of Contents

1. [Project Overview](#project-overview)
2. [Key Features](#key-features)
3. [Repository Layout](#repository-layout)
4. [QuickÂ Start](#quick-start)
5. [Configuration](#configuration)
6. [Running the Tools](#running-the-tools)
   - [CLI](#cli-usage)
   - [DesktopÂ GUI](#desktop-gui)
7. [Data Model](#data-model)
8. [Extending the Project](#extending-the-project)
9. [TroubleshootingÂ &Â FAQ](#troubleshooting--faq)
10. [Roadmap](#roadmap)
11. [AcknowledgementsÂ &Â License](#acknowledgements--license)

---

## Project Overview

This project automates the discovery of Leeds lecturers who publishâ€”or appear to have expertiseâ€”in ArtificialÂ Intelligence. It **scrapes**:

- **UniversityÂ ofÂ Leeds staff pages** to collect lecturer names, positions, expertise, profile URLs and *optional* GoogleÂ Scholar links.
- **GoogleÂ Scholar** to find each lecturerâ€™s profile, publication titles and declared interests.

All data are stored in **MongoDB**, and the text of interests / publications is passed through a twoâ€‘stage *zeroâ€‘shot* classifier (Metaâ€™s `facebook/bart-large-mnli`) + KeyBERT keyâ€‘phrase extraction to determine AIâ€‘relevance.

A **PyQt5 desktop application** offers a pointâ€‘andâ€‘click interface on top of the same codebase, while a simple **commandâ€‘line interface** is available for servers or scheduled runs.

---

## Key Features

| Area                   | Highlights                                                                                                                                                       |
| ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Scraping**           | Rotating proxies & headers, GoogleÂ CAPTCHA detection, pagination handling for Scholar publications.                                                              |
| **AIÂ SkillÂ Detection** | 2â€‘stage zeroâ€‘shot classification ("ArtificialÂ Intelligence" â†‘) plus fineâ€‘grained negative labels; KeyBERT keyâ€‘phrase extraction; duplicate phrase pruning.       |
| **Database**           | MongoDB upsert; easy reâ€‘scrape (forceâ€‘update) flags; a single `lecturers` collection holds everything.                                                           |
| **Interface**          | *CLI* workflow for automation; *GUI* (PyQt5) with background worker threads so windows never freeze; rich filters (school, AIâ€‘only, skill search, AND/OR logic). |
| **Extensibility**      | School URLs live in one dictionary (`department.py`); classifier labels & thresholds tunable in `ai_classifiers.py`.                                             |

---

## Repository Layout

```
â”œâ”€â”€ ai_classifiers.py   # Zeroâ€‘shot + KeyBERT utilities
â”œâ”€â”€ department.py       # Mapping of Leeds schools â†’ faculty â†’ staffâ€‘list URL
â”œâ”€â”€ database.py         # MongoDB connection helper
â”œâ”€â”€ interface.py        # PyQt5 desktop app
â”œâ”€â”€ main.py             # CLI entryâ€‘point (menu)
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ scraper.py          # Leeds staffâ€‘page scraper
â”œâ”€â”€ scholar_scraper.py  # GoogleÂ Scholar scraper
â””â”€â”€ utils.py            # Shared helpers (proxy rotation, text utilsâ€¦)
```

---

## QuickÂ Start

### 1Â Â·Â Prerequisites

- PythonÂ â‰¥Â 3.10 (tested onâ€¯3.11)
- A MongoDB instance (Atlas or local). The default URI **must be changed**â€”see Â§Â [Configuration](#configuration).
- Chrome/Firefox (only for your own browsing; scraping uses `requests`).

### 2Â Â·Â Clone & Install

```bash
# 0) clone
$ git clone https://github.com/jsv832/UoL-Expert-Finder.git
$ cd leeds-ai-scraper

# 1) create and activate a virtual environment (recommended)
$ python -m venv .venv
$ source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 2) install Python deps
$ pip install -r requirements.txt

# 3) download SpaCy model (first run only)
$ python -m spacy download en_core_web_sm
```

---


> **âš ï¸Â Security note**: `database.py` currently contains a hardâ€‘coded `MONGO_URI`.
> It should be moved to environment variable but to make it easier to have access to project it visible

---

## Running the Tools

### CLIÂ Usage

Run the menuâ€‘driven CLI:

```bash
$ python main.py
```

Youâ€™ll be asked to:

1. **Scrape Leeds staff pages** (`scraper.py`).
2. **Scrape Googleâ€¯Scholar** (`scholar_scraper.py`).
3. **Run both in sequence** .

Each scraper can optionally **forceâ€‘update** existing lecturer documents (otherwise they are skipped if already processed).

### DesktopÂ GUI

```bash
$ python interface.py
```

The window offers three tabs:

- **RunÂ Scrapers** â€“ choose school, forceâ€‘update, background progress.
- **ProfessorÂ List** â€“ browse, filter by school/name/AIâ€‘only, doubleâ€‘click to see full details.
- **SkillÂ Search** â€“ build an AND/OR query of AI skill keywords to find matching lecturers.



## Data Model

All documents reside in collection ``; key fields include:

| Field                   | Type       | Description                                     |
| ----------------------- | ---------- | ----------------------------------------------- |
| `_id`                   | String     | Â Primary key; the lecturerâ€™s staffâ€‘profile URL. |
| `name`                  | String     | Lecturerâ€™s full name.                           |
| `school` / `department` | String     | University of Leeds organisational units.       |
| `skills_expertise`      | List[str]  | Staff page â€œAreas of expertiseâ€.                |
| `research_papers`       | List[dict] | Publications scraped from Leeds pages.          |
| `scholar_profile`       | String     | GoogleÂ Scholar URL (if found).                  |
| `scholar_publications`  | List[dict] | All Scholar publications (titleÂ +Â year).        |
| `ai_skills`             | List[str]  | **Final deduplicated AIâ€‘skill keyâ€‘phrases**.    |
| `is_ai_lecturer`        | Bool       | True if any AI skills were detected.            |
| `scholar_processed`     | Bool       | Flag to avoid reâ€‘processing.                    |

---

## Extending the Project

- **Add new schools** â€“ Edit `department.py`.
- **Add Scripts** â€“ Edit `utils.py`.
- **Tweak AI detection** â€“ Adjust thresholds/constants in `ai_classifiers.py`.
- **Improve AI classifier** - Fine Tune model instead of zero-shot `ai_classifiers.py`/
- **Headless scraping** â€“ Integrate Selenium/Playwright if JavaScriptâ€‘rendered pages require it.

## TroubleshootingÂ &Â FAQ

| Problem                              | Solution                                                                                  |
| ------------------------------------ | ----------------------------------------------------------------------------------------- |
| *Google scholar blocks with CAPTCHA* | Wait, change IP/proxy, lower request rate (`utils.select_proxy_and_headers`).             |
| *GUI freezes*                        | Long scrapes run inside `ScrapeWorker` threads; ensure PyQt â‰¥Â 5.15.                       |
| *Empty AIâ€‘skills*                    | Check `AI_RELATED_THRESHOLD` (defaultÂ 0.60); raise/lower to tune recall.                  |
| *Duplicate lecturers*                | The `_id` is the Leeds profile URL â€“ duplicates shouldnâ€™t appear unless the site changes. |


## AcknowledgementsÂ &Â License

This project bundles openâ€‘source work, notably:

- **SpaCy** (MIT), **Transformers** (Apacheâ€‘2.0), **KeyBERT** (Apacheâ€‘2.0)
- *facebook/bart-large-mnli* model Â©Â Meta AI
- University of Leeds staffÂ pages and Google Scholar Â©Â their respective owners.
